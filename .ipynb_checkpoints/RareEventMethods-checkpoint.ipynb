{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import impute\n",
    "import PMLE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dfs and split into train and val\n",
    "identity = pd.read_csv('train_identity.csv')\n",
    "trans = pd.read_csv('train_transaction.csv')\n",
    "df = identity.merge(trans, on='TransactionID')\n",
    "df = pd.get_dummies(df,drop_first=True)\n",
    "\n",
    "#drop columns that are all NA or have only one value\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        if (df[col].isna().sum()==df.shape[0]) | (df[col].min()==df[col].max()):\n",
    "            df.drop(col,axis=1,inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "#Obtain indices for non-fraud rows\n",
    "normal_inds = df[df.isFraud==0].index\n",
    "\n",
    "#Train test split\n",
    "X = df.drop('isFraud',axis=1)\n",
    "y = df['isFraud']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=.2,random_state=0)\n",
    "\n",
    "#Establish floors and ceilings for columns\n",
    "limited_range = {}\n",
    "for col in X_train.columns:\n",
    "    if (X_train[col].nunique()<10)|(X_train[col].min()==0):\n",
    "        limited_range[col]=(X_train[col].min(),X_train[col].max())\n",
    "\n",
    "#Impute missing values and standardize\n",
    "X_train, problems = impute.impute_missing_values(X_train,col_floor_ceiling_dict=limited_range)\n",
    "X_val, problems = impute.impute_missing_values(X_train,col_floor_ceiling_dict=limited_range)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_val = sc.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalized Maximum Likelihood Estimation\n",
    "Using Firth's Logistic Regression with Intercept Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get estimates using Firth's Logistic Regression with Intercept Correction\n",
    "FLIC = PMLE.Firth_Logit(FLIC=True)\n",
    "FLIC.fit(X_train,y_train)\n",
    "FLIC_train_preds = FLIC.predict_proba(X_train)\n",
    "FLIC_val_preds = FLIC.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE Dataset Resampling\n",
    "Estimation with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get XGBoost estimate of SMOTE dataset\n",
    "smote = SMOTE()\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train,y_train)\n",
    "X_val_smote, y_val_smote = smote.fit_resample(X_val,y_val)\n",
    "\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n",
    "                    silent=True, nthread=1)\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "xgb_gs = GridSearchCV(xgb,params,cv=5)\n",
    "xgb_gs.fit(X_train_smote,y_train_smote)\n",
    "smote_train_preds = xgb_gs.predict_proba(X_train_smote)\n",
    "smote_train_preds = xgb_gs.predict_proba(X_val_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Anomaly Detection\n",
    "With standard, regularized and denoising autoencoders using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert non-fraud rows to pytorch tensor\n",
    "normal_train = normal_train[normal_inds,:]\n",
    "normal_torch = torch.from_numpy(val_x).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create autoencoder class and training function\n",
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_features,hidden_nodes,dropout=False):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.n_features=n_features\n",
    "        self.n_hidden = hidden_nodes\n",
    "        self.encoder = nn.Linear(n_features,hidden_nodes)\n",
    "        self.decoder = nn.Linear(hidden_nodes,n_features)\n",
    "        self.output_layer = nn.Linear(n_features,n_features)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.dropout!=False:\n",
    "            x = F.relu(F.dropout(self.encoder(x)))\n",
    "        else:\n",
    "            x = F.relu(self.encoder(x))\n",
    "        self.hidden_layer=x\n",
    "        x = F.relu(self.decoder(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_autoencoder(model, dataset,loss_func,optimizer, epochs=5, batch_size=32, \n",
    "                      lr=1e-3,noise=False,noise_factor=None):\n",
    "    torch.manual_seed(0)\n",
    "    rows = dataset.shape[0]\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "    outputs = []\n",
    "    for epoch in range(epochs):\n",
    "        counter = 0\n",
    "        print('\\nEPOCH:{}\\n'.format(epoch+1))\n",
    "        for batch in train_loader:\n",
    "            \n",
    "            if noise == True:\n",
    "                batch = batch + noise_factor * torch.randn(*batch.shape)\n",
    "            batch = torch.autograd.Variable(batch)\n",
    "            optimizer.zero_grad()\n",
    "            recon = model(batch)\n",
    "            \n",
    "            loss = loss_func(recon, batch)\n",
    "            if (counter%50==0):\n",
    "                print('Batch Loss: {:.4f}'.format(float(loss)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            counter+=1\n",
    "        epoch_loss = loss_func(model(dataset), dataset)\n",
    "        print('Epoch {}: Loss: {:.4f}'.format(epoch+1, float(epoch_loss)))\n",
    "        \n",
    "        outputs.append((epoch, X, recon),)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard autoencoder\n",
    "ae1 = AutoEncoder(2595,2595)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "train_autoencoder(model=ae1,\n",
    "                   dataset=output2,\n",
    "                   loss_func=loss_func,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=512,\n",
    "                   epochs=10)\n",
    "output1 = ae(X_train_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout autoencoder\n",
    "ae1 = AutoEncoder(2595,4000,dropout=0.5)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(ae.parameters(), lr=1e-3,momentum=0.9,nesterov=True)\n",
    "train_autoencoder(model=ae1,\n",
    "                   dataset=normal_train_torch,\n",
    "                   loss_func=loss_func,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=1024,\n",
    "                   epochs=40)\n",
    "output2 = ae(output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L2 autoencoder\n",
    "ae2 = AutoEncoder(2595,2595)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3,weight_decay=0.2)\n",
    "train_autoencoder(model=ae2,\n",
    "                   dataset=output2,\n",
    "                   loss_func=loss_func,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=512,\n",
    "                   epochs=10)\n",
    "output3 = ae(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard autoencoder\n",
    "ae3 = AutoEncoder(2595,1000)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "train_autoencoder(model=ae3,\n",
    "                   dataset=output2,\n",
    "                   loss_func=loss_func,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=512,\n",
    "                   epochs=10)\n",
    "output4 = ae(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denoising autoencoder\n",
    "ae4 = AutoEncoder(2595,2595)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "train_autoencoder(model=ae4,\n",
    "                   dataset=output1,\n",
    "                   loss_func=loss_func,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=512,\n",
    "                   epochs=5,\n",
    "                   noise=True,\n",
    "                  noise_factor=0.25)\n",
    "output5 = ae(output4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard autoencoder\n",
    "ae5 = AutoEncoder(2595,500)\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "train_autoencoder(model=ae5,\n",
    "                   dataset=output2,\n",
    "                   loss_func=loss_func,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=512,\n",
    "                   epochs=10)\n",
    "output6 = ae(output5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 Regularization autoencoder\n",
    "def L1_loss(recon,inputs):\n",
    "    MSELoss = nn.MSELoss()\n",
    "    loss = MSELoss(recon,inputs)\n",
    "    for param in ae.parameters():\n",
    "        loss += torch.sum(torch.abs(param))\n",
    "    return loss\n",
    "\n",
    "ae6 = AutoEncoder(2595,2595)\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n",
    "train_autoencoder(model=ae6,\n",
    "                   dataset=output3,\n",
    "                   loss_func=L1_Loss,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=512,\n",
    "                   epochs=10)\n",
    "output7 = ae(output6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output7.numpy()\n",
    "val_output = val_output7.numpy()\n",
    "anomaly_score = np.mean(np.power(output - X_train, 2), axis=1)\n",
    "val_anomaly_score = np.mean(np.power(val_output - X_val, 2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble results\n",
    "Estimation using FLIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_X_train = pd.DataFrame(None)\n",
    "ensemble_X_train['anomaly_score'] = anomaly_score\n",
    "ensemble_X_train['FLIC'] = FLIC_train_preds\n",
    "ensemble_X_train['smote'] = smote_train_preds\n",
    "ensemble_X_val = pd.DataFrame(None)\n",
    "ensemble_X_val['anomaly_score'] = val_anomaly_score\n",
    "ensemble_X_val['FLIC'] = FLIC_val_preds\n",
    "ensemble_X_val['smote'] = smote_val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIC.fit(ensemble_X_train,y_train)\n",
    "preds = FLIC.predict_proba(ensemble_X_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
