{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remycanario/anaconda3/lib/python3.7/site-packages/statsmodels/compat/pandas.py:23: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import FirthLogit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import datetime\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import log_loss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import random\n",
    "from keras import regularizers\n",
    "from keras import models,layers,optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import testingFirth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "time_df = pd.DataFrame(df.Time)\n",
    "time_df['hour_dummy']=(time_df.Time/(60*60)).round().astype(str)\n",
    "time_df['day_dummy']=(time_df.Time/(60*60*24)).round().astype(str)\n",
    "time_df = pd.get_dummies(time_df,drop_first=True).drop('Time',axis=1)\n",
    "df.drop('Time',axis=1,inplace=True)\n",
    "\n",
    "df['cents'] = df.Amount.apply(lambda x: int(str(x).split('.')[1]))\n",
    "df.Amount[df.Amount!=0] = np.log(df.Amount[df.Amount!=0])\n",
    "\n",
    "X = df.drop('Class',axis=1)\n",
    "y = df.Class\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2,random_state=0)\n",
    "time_train, time_val = train_test_split(time_df,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain bagged logistic regression with penalized MLE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bagged_PMLE_results(X_train,y_train,functions,function_labels,iterations=20):\n",
    "    train = dict.fromkeys(function_labels,np.zeros(X_train.shape[0]))\n",
    "    val = dict.fromkeys(function_labels,np.zeros(X_val.shape[0]))\n",
    "    for i in range(iterations):\n",
    "        print('Epoch', i+1)\n",
    "        X = X_train[y_train==1].sample(frac=0.05).append(X_train[y_train==0].sample(frac=0.05))\n",
    "        y = y_train.loc[X.index]\n",
    "        for j in range(len(functions)):\n",
    "            print(function_labels[i])\n",
    "            functions[j].fit(X,y)\n",
    "            train[function_labels[j]] = ((i)*train[function_labels[j]] + functions[j].predict_proba(X_train))/(i+1)\n",
    "            val[function_labels[j]] =  ((i)*val[function_labels[j]] + functions[j].predict_proba(X_val))/(i+1)\n",
    "    return train, val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firth = PMLE.PMLE.Firth_Logit(num_iters=125,lr=0.05, metric='recall_score', readout_rate=10)\n",
    "FLIC = PMLE.PMLE.Firth_Logit(num_iters=125,lr=0.05,FLIC=True, metric='recall_score', readout_rate=10)\n",
    "t_firth = PMLE.PMLE.Firth_Logit(num_iters=125,lr=0.05,lmbda=0.01, metric='recall_score', readout_rate=10)\n",
    "\n",
    "functions = [firth, FLIC, t_firth]\n",
    "function_labels = ['firth','FLIC','t_firth']\n",
    "\n",
    "train_results, val_results = get_bagged_PMLE_results(X_train,y_train,functions,function_labels,iterations=1)\n",
    "\n",
    "train_results = pd.DataFrame.from_dict(train_results)\n",
    "train_results.index = X_train.index\n",
    "val_results = pd.DataFrame.from_dict(val_results)\n",
    "val_results.index = X_val.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = X_train.sample(frac=0.05)\n",
    "test_y = y_train.loc[test_X.index]\n",
    "X_val = X_val.sample(frac=0.05)\n",
    "tf = testingFirth.testingFirth(num_iters=1,lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "firth_results, flac_results, flic_results = tf.fit(test_X,test_y,X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Anomaly Detection\n",
    "### With undercomplete, regularized and denoising autoencoders using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify non-fraud rows\n",
    "normal_inds = y_train[y_train==0].reset_index().index\n",
    "\n",
    "#Standardize dfs\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_val_sc = sc.fit_transform(X_val)\n",
    "n_features = X_train_sc.shape[1]\n",
    "\n",
    "#Convert non-fraud rows to pytorch tensor\n",
    "normal_train = X_train_sc[normal_inds,:]\n",
    "normal_torch = torch.from_numpy(normal_train,).type(torch.FloatTensor)\n",
    "\n",
    "#Convert train and val sets to pytorch tensor\n",
    "train_torch = torch.from_numpy(X_train_sc).type(torch.FloatTensor)\n",
    "val_torch = torch.from_numpy(X_val_sc).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create autoencoder class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_features,hidden_nodes,dropout=None,VAE=False):\n",
    "        \n",
    "        '''PARAMETERS\n",
    "           n_features: number of X variables in dataset\n",
    "           hidden_nodes: number of nodes in hidden layer\n",
    "           dropout: fraction of nodes to dropout (0 < dropout <1)'''\n",
    "        \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.n_features=n_features\n",
    "        self.n_hidden = hidden_nodes\n",
    "        self.encoder = nn.Linear(n_features,hidden_nodes)\n",
    "        self.decoder = nn.Linear(hidden_nodes,n_features)\n",
    "        self.output_layer = nn.Linear(n_features,n_features)\n",
    "        self.dropout = dropout\n",
    "        self.best_recon = None\n",
    "        \n",
    "        \n",
    "    def forward (self,x):\n",
    "        if self.dropout!=None:\n",
    "            x = F.relu(F.dropout(self.encoder(x),p=self.dropout))\n",
    "        else:\n",
    "            x = F.relu(self.encoder(x))\n",
    "        self.hidden_layer=x\n",
    "        x = F.relu(self.decoder(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataset, loss_func, optimizer, epochs=100, batch_size=1024,  \n",
    "                      validation_tensor=None,y_val=None, lr_rate_scheduler = None, noise_factor=None, \n",
    "                      random_seed=None, MSE_stopping_threshold=0):\n",
    "        '''PARAMETERS\n",
    "           model: instantiated autoencoder\n",
    "           dataset: torch tensor of X variables\n",
    "           loss_func: instantiated loss function\n",
    "           optimizer: instantiated optimizer\n",
    "           validation_tensor: torch tensor of validation X variables\n",
    "           y_val: numpy array of validation y values\n",
    "           epochs: number of epochs\n",
    "           lr_rate_scheduler: instantiated PyTorch learning rate scheduler\n",
    "           batch_size: batch size\n",
    "           noise_factor: magnitude of noise added to data\n",
    "             for a denoising autoencoder (0 < noise_factor <=1)\n",
    "           random_seed: random_seed\n",
    "           stopping_MSE_threshold: MSE value after which autoencoder stops training'''\n",
    "\n",
    "        \n",
    "        #Set up\n",
    "        if random_seed!=None:\n",
    "            torch.manual_seed(random_seed)\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   shuffle=True)\n",
    "\n",
    "        if type(validation_tensor)==torch.Tensor:\n",
    "            val = True\n",
    "            val_numpy = validation_tensor.detach().numpy()\n",
    "        else:\n",
    "            val = False\n",
    "\n",
    "        readout_batch_interval = 0.25*(dataset.shape[0]/batch_size)//1\n",
    "        \n",
    "        #Training\n",
    "        for epoch in range(epochs):\n",
    "            counter = 0\n",
    "            print('\\n\\033[1mEpoch {}\\033[0m\\n'.format(epoch+1))\n",
    "            for batch in train_loader:\n",
    "\n",
    "                if noise_factor!=None:\n",
    "                    batch = batch + noise_factor * torch.randn(*batch.shape)\n",
    "                batch = torch.autograd.Variable(batch)\n",
    "                optimizer.zero_grad()\n",
    "                recon = model(batch)\n",
    "                loss = loss_func(recon, batch)\n",
    "                if counter%readout_batch_interval==0:\n",
    "                    print('Batch {} Loss: {:.4f}'.format(counter, float(loss)))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                counter+=1\n",
    "            \n",
    "            #Readout for each epoch\n",
    "            if epoch==0:\n",
    "                epoch_loss = loss_func(model(dataset), dataset)\n",
    "                print('\\nEPOCH {} LOSS: {:.4f}'.format(epoch+1, float(epoch_loss)))\n",
    "            else:\n",
    "                old_epoch_loss = epoch_loss\n",
    "                epoch_loss = loss_func(model(dataset), dataset)\n",
    "                print('\\nEPOCH {} LOSS: {:.4f}'.format(epoch+1, float(epoch_loss)))\n",
    "                \n",
    "            if val == True:\n",
    "                val_output = model(validation_tensor).detach().numpy()\n",
    "                reconstruction_error = np.sqrt(np.power(val_output - val_numpy, 2)).sum(axis=1)\n",
    "                reconstruction_error = sc.fit_transform(reconstruction_error.reshape(-1, 1))\n",
    "                sklogit = LogisticRegression()\n",
    "                if epoch==0:\n",
    "                    sklogit.fit(reconstruction_error,y_val)\n",
    "                    preds = sklogit.predict(reconstruction_error)\n",
    "                    score = recall_score(y_val,preds)\n",
    "                    model.best_recon=model.parameters()\n",
    "                    model.best_pr = score\n",
    "                    print('\\nReconstruction error recall: {:.4f}'.format(score))\n",
    "                else:\n",
    "                    old_score = score\n",
    "                    sklogit.fit(reconstruction_error,y_val)\n",
    "                    preds = sklogit.predict(reconstruction_error)\n",
    "                    score = recall_score(y_val,preds)\n",
    "                    if score<old_score:\n",
    "                        model.best_recon=model.parameters()\n",
    "                        model.best_recall = score\n",
    "                    print('\\nReconstruction error recall {:.4f}'.format(score))\n",
    "                    print('Change: {:.4f}%'.format(float((score-old_score)/old_score)))\n",
    "            if type(scheduler)==torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
    "                scheduler.step(score) \n",
    "            if epoch_loss<=MSE_stopping_threshold:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder I: Undercomplete with Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate encoder and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae1 = AutoEncoder(n_features,int(n_features*1.5//1),dropout=0.3)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with high learning rate\n",
    "optimizer = torch.optim.SGD(ae1.parameters(), lr=0.02, momentum=0.9,nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.9, patience=4, verbose=True)\n",
    "train_autoencoder(model=ae1,\n",
    "                   dataset=normal_torch,\n",
    "                   loss_func=loss_func,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=256,\n",
    "                   epochs=100,\n",
    "                   lr_rate_scheduler=scheduler,\n",
    "                   validation_tensor=val_torch,\n",
    "                   y_val = y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = ae1(normal_torch)\n",
    "train_output1 = ae1(train_torch)\n",
    "val_output1 = ae1(val_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder II: L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae2 = AutoEncoder(n_features,n_features)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(ae2.parameters(), lr=0.2, weight_decay=0.25)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2, verbose=True)\n",
    "train_autoencoder(model=ae2,\n",
    "                   dataset=output1,\n",
    "                   loss_func=loss_func,\n",
    "                   optimizer=optimizer,\n",
    "                   batch_size=256,\n",
    "                   epochs=500,\n",
    "                   validation_tensor=val_torch,\n",
    "                   lr_rate_scheduler=scheduler,\n",
    "                   y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2 = ae2(output1)\n",
    "train_output2 = ae2(train_output1)\n",
    "val_output2 = ae2(val_output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder III: Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae3 = AutoEncoder(n_features,n_features)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(ae3.parameters(), lr=0.2,momentum=0.9,nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2, verbose=True)\n",
    "train_autoencoder(model=ae3,\n",
    "                  dataset=output2,\n",
    "                  loss_func=loss_func,\n",
    "                  optimizer=optimizer,\n",
    "                  batch_size=512,\n",
    "                  epochs=10,\n",
    "                  noise_factor=0.9,\n",
    "                  validation_tensor=val_torch,\n",
    "                  y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = ae3(output2)\n",
    "train_output3 = ae3(train_output2)\n",
    "val_output3 = ae3(val_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder IV: L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae4 = AutoEncoder(n_features,n_features)\n",
    "\n",
    "def L1_loss(recon,inputs):\n",
    "    MSELoss = nn.MSELoss()\n",
    "    loss = MSELoss(recon,inputs)\n",
    "    for param in ae4.parameters():\n",
    "        loss += lmbda*torch.sum(torch.abs(param))\n",
    "    return loss\n",
    "\n",
    "loss_func = L1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbda=0.25\n",
    "optimizer = torch.optim.SGD(ae4.parameters(), lr=0.02,momentum=0.9,nesterov=True)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2, verbose=True)\n",
    "train_autoencoder(model=ae4,\n",
    "                  dataset=output3,\n",
    "                  loss_func=loss_func,\n",
    "                  optimizer=optimizer,\n",
    "                  batch_size=512,\n",
    "                  epochs=200,\n",
    "                  validation_tensor=val_torch,\n",
    "                  y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "output4 = ae4(output3)\n",
    "train_output4 = ae4(train_output3)\n",
    "val_output4 = ae4(val_output3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder V: Kitchen Sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae5 = AutoEncoder(n_features,int(n_features//4),dropout=0.5)\n",
    "loss_func = L1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(ae5.parameters(), lr=0.02, weight_decay=0.5)\n",
    "train_autoencoder(model=ae5,\n",
    "                  dataset=output4,\n",
    "                  loss_func=loss_func,\n",
    "                  optimizer=optimizer,\n",
    "                  batch_size=1024,\n",
    "                  epochs=200,\n",
    "                  noise_factor=0.5,\n",
    "                  validation_tensor=val_torch,\n",
    "                  y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "output5 = ae5(output4)\n",
    "train_output5 = ae5(train_output4)\n",
    "val_output5 = ae5(val_output4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add reconstruction score to DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_output = train_output5.detach().numpy()\n",
    "final_val_output = val_output5.detach().numpy()\n",
    "\n",
    "#calculate reconstruction score\n",
    "train_reconstruction_score = np.power(X_train_sc - final_train_output,2).sum(axis=1)\n",
    "train_reconstruction score = train_reconstruction_score/(train_reconstruction_score.max()-train_reconstruction_score.min())\n",
    "\n",
    "#normalize\n",
    "val_reconstruction_score = np.power(X_val_sc - final_val_output,2).sum(axis=1)\n",
    "val_reconstruction score = val_reconstruction_score/(val_reconstruction_score.max()-val_reconstruction_score.min())\n",
    "\n",
    "train_results['recon_score']=train_reconstruction_score\n",
    "val_results['recon_score']= val_reconstruction_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L2 regularization on SMOTE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(n_jobs=-1)\n",
    "smote_train_X, smote_train_y = smote.fit_resample(X_train,y_train)\n",
    "sklearn = LogisticRegression()\n",
    "sklearn.fit(smote_train_X,smote_train_y)\n",
    "train_results['smote']=sklearn.predict_proba(X_train)[:,1]\n",
    "val_results['smote']=sklearn.predict_proba(X_val)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class-Weighted MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add time data to columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.join(time_df)\n",
    "X_val = X_val.join(time_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_keras = to_categorical(y_train)\n",
    "y_val_keras = to_categorical(y_val)\n",
    "class_weights = {0:1,1:1/y_train.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_keras = sc.fit_transform(X_train)\n",
    "X_val_keras = sc.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set checkpoint and learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(monitor='loss',\n",
    "                                factor=0.8, \n",
    "                                patience=10, \n",
    "                                verbose=1,\n",
    "                                mode='auto', \n",
    "                                min_delta=0.0001, \n",
    "                                cooldown=0, \n",
    "                                min_lr=0)\n",
    "checkpoint = ModelCheckpoint('checkpoint.best.hdf5',  verbose=1, save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and compile MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = models.Sequential()\n",
    "nn.add(layers.Dropout(.3))\n",
    "nn.add(layers.Dense(128, input_shape=(val_results.shape[1],), activation='relu'))\n",
    "nn.add(layers.Dense(64,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "nn.add(layers.Dropout(.3))\n",
    "nn.add(layers.Dense(32,activation='relu'))\n",
    "nn.add(layers.Dense(16,activation='relu'))\n",
    "nn.add(layers.Dropout(.3))\n",
    "nn.add(layers.Dense(8,activation='relu'))\n",
    "nn.add(layers.Dense(4,activation='relu'))\n",
    "nn.add(layers.Dense(2,activation='sigmoid'))\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = models.Sequential()\n",
    "nn.add(layers.Dropout(.3))\n",
    "nn.add(layers.Dense(64,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "nn.add(layers.Dropout(.3))\n",
    "nn.add(layers.Dense(32,activation='relu'))\n",
    "nn.add(layers.Dense(16,activation='relu'))\n",
    "nn.add(layers.Dropout(.3))\n",
    "nn.add(layers.Dense(8,activation='relu'))\n",
    "nn.add(layers.Dense(4,activation='relu'))\n",
    "nn.add(layers.Dense(2,activation='sigmoid'))\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(train_results.values, \n",
    "       y_train_keras, \n",
    "       epochs=1000, \n",
    "       class_weight=class_weights,\n",
    "       batch_size=512,\n",
    "       validation_data=(val_results.values,y_val_keras),\n",
    "       callbacks=[scheduler,checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_nn = models.Sequential()\n",
    "results_nn.add(layers.Dense(128, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "results_nn.add(layers.Dense(64,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "results_nn.add(layers.Dense(32,activation='relu'))\n",
    "results_nn.add(layers.Dense(16,activation='relu'))\n",
    "results_nn.add(layers.Dense(8,activation='relu'))\n",
    "results_nn.add(layers.Dense(4,activation='relu'))\n",
    "results_nn.add(layers.Dense(2,activation='sigmoid'))\n",
    "results_nn.load_weights('checkpoint.best.hdf5')\n",
    "results_nn.compile(loss='binary_crossentropy',optimizer='adam')\n",
    "\n",
    "val_preds = results_nn.predict(X_val_keras)\n",
    "train_preds = results_nn.predict(X_train_keras)\n",
    "\n",
    "train_results['mlp'] = train_preds[:,1]\n",
    "val_results['mlp'] = val_preds[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all results through FLIC to obtain final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e9e4cb00>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "recalls = []\n",
    "auc_rocs = []\n",
    "log_losses =[]\n",
    "for col in val_results.columns:\n",
    "    recalls.append(recall_score(y_val,val_results[col].round()))\n",
    "    auc_rocs.append(roc_auc_score(y_val,val_results[col].round()))\n",
    "    log_losses.append(log_loss(y_val,val_results[col]))\n",
    "    \n",
    "estimators = list(val_results.columns)\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(10,7.5))\n",
    "r1 = np.arange(len(recalls))\n",
    "r2 = [i + width for i in r1]\n",
    "r3 = [i + width for i in r2]\n",
    "\n",
    "plt.bar(r1, recalls, color='red', width=width, edgecolor='white', label='Recall Score')\n",
    "plt.bar(r2, auc_rocs, color='blue', width=width, edgecolor='white', label='AUC ROC')\n",
    "plt.bar(r3, log_losses, color='yellow', width=width, edgecolor='white', label='Log Loss')\n",
    "\n",
    "plt.xlabel('Estimate', fontweight='bold')\n",
    "plt.xticks([i + width for i in range(len(recalls))], estimators)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLIC = PMLE.PMLE.Firth_Logit(num_iters=100,lr=0.05,FLIC=True, metric='recall_score', readout_rate=10)\n",
    "train = np.zeros(X_train.shape[0])\n",
    "val = np.zeros(X_val.shape[0])\n",
    "for i in range(20):\n",
    "    print('Epoch:',i+1)\n",
    "    X = train_results[y_train==1].sample(frac=0.05).append(train_results[y_train==0].sample(frac=0.05))\n",
    "    y = y_train.loc[X.index]\n",
    "    FLIC.fit(X,y)\n",
    "    train = ((i)*train + FLIC.predict_proba(train_results))/(i+1)\n",
    "    val =  ((i)*val + FLIC.predict_proba(val_results))/(i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final validation set area under the ROC curve score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9884146871274243"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_val,val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
